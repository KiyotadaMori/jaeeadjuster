{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **jaeeadjuster**\n",
        "This is a forced alignment tool for adjusting duration of words between Native speaker's English & Japanese-accented English.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZMdK_6xXYTUm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z6IydpCEEYLq",
        "outputId": "4f9a514b-f819-4b64-ca88-dcd1b3812ff9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/jianfch/stable-ts.git\n",
            "  Cloning https://github.com/jianfch/stable-ts.git to /tmp/pip-req-build-6w0tip9w\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/jianfch/stable-ts.git /tmp/pip-req-build-6w0tip9w\n",
            "  Resolved https://github.com/jianfch/stable-ts.git to commit 5ac6f5e11eedddc7bedf728c70eb19b303b4b63c\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from stable-ts==2.11.3) (1.23.5)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from stable-ts==2.11.3) (2.0.1+cu118)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.10/dist-packages (from stable-ts==2.11.3) (2.0.2+cu118)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from stable-ts==2.11.3) (4.66.1)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.10/dist-packages (from stable-ts==2.11.3) (10.1.0)\n",
            "Collecting transformers>=4.19.0 (from stable-ts==2.11.3)\n",
            "  Downloading transformers-4.33.3-py3-none-any.whl (7.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m29.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ffmpeg-python==0.2.0 (from stable-ts==2.11.3)\n",
            "  Downloading ffmpeg_python-0.2.0-py3-none-any.whl (25 kB)\n",
            "Collecting openai-whisper==20230918 (from stable-ts==2.11.3)\n",
            "  Downloading openai-whisper-20230918.tar.gz (794 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m794.3/794.3 kB\u001b[0m \u001b[31m45.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.10/dist-packages (from ffmpeg-python==0.2.0->stable-ts==2.11.3) (0.18.3)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20230918->stable-ts==2.11.3) (2.0.0)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20230918->stable-ts==2.11.3) (0.56.4)\n",
            "Collecting tiktoken==0.3.3 (from openai-whisper==20230918->stable-ts==2.11.3)\n",
            "  Downloading tiktoken-0.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m25.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken==0.3.3->openai-whisper==20230918->stable-ts==2.11.3) (2023.6.3)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken==0.3.3->openai-whisper==20230918->stable-ts==2.11.3) (2.31.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->openai-whisper==20230918->stable-ts==2.11.3) (3.27.4.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->openai-whisper==20230918->stable-ts==2.11.3) (3.12.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->openai-whisper==20230918->stable-ts==2.11.3) (16.0.6)\n",
            "Collecting huggingface-hub<1.0,>=0.15.1 (from transformers>=4.19.0->stable-ts==2.11.3)\n",
            "  Downloading huggingface_hub-0.17.3-py3-none-any.whl (295 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.0/295.0 kB\u001b[0m \u001b[31m31.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.19.0->stable-ts==2.11.3) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.19.0->stable-ts==2.11.3) (6.0.1)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers>=4.19.0->stable-ts==2.11.3)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m89.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers>=4.19.0->stable-ts==2.11.3)\n",
            "  Downloading safetensors-0.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m35.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->stable-ts==2.11.3) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->stable-ts==2.11.3) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->stable-ts==2.11.3) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->stable-ts==2.11.3) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers>=4.19.0->stable-ts==2.11.3) (2023.6.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken==0.3.3->openai-whisper==20230918->stable-ts==2.11.3) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken==0.3.3->openai-whisper==20230918->stable-ts==2.11.3) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken==0.3.3->openai-whisper==20230918->stable-ts==2.11.3) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken==0.3.3->openai-whisper==20230918->stable-ts==2.11.3) (2023.7.22)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->stable-ts==2.11.3) (2.1.3)\n",
            "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->openai-whisper==20230918->stable-ts==2.11.3) (0.39.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from numba->openai-whisper==20230918->stable-ts==2.11.3) (67.7.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->stable-ts==2.11.3) (1.3.0)\n",
            "Building wheels for collected packages: stable-ts, openai-whisper\n",
            "  Building wheel for stable-ts (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for stable-ts: filename=stable_ts-2.11.3-py3-none-any.whl size=57573 sha256=5d6c16c333a36a4f2e17d6f39b66d8a858de54e9b0f855bc56d59b70cc82dc64\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-r9p39wmv/wheels/5a/48/64/a463d57ac05105e1692e3649ca76cea98a8867262d7b32dd86\n",
            "  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for openai-whisper: filename=openai_whisper-20230918-py3-none-any.whl size=798399 sha256=b6e5e3a64273fec8ae22987b573c5cd5e576d6c42c6f050546e93426937b7593\n",
            "  Stored in directory: /root/.cache/pip/wheels/5d/37/b1/9aea93201fe91e3561719120da92cc23e77b7ef6f3d0d9491a\n",
            "Successfully built stable-ts openai-whisper\n",
            "Installing collected packages: tokenizers, safetensors, ffmpeg-python, tiktoken, huggingface-hub, transformers, openai-whisper, stable-ts\n",
            "Successfully installed ffmpeg-python-0.2.0 huggingface-hub-0.17.3 openai-whisper-20230918 safetensors-0.3.3 stable-ts-2.11.3 tiktoken-0.3.3 tokenizers-0.13.3 transformers-4.33.3\n",
            "Collecting alkana\n",
            "  Downloading alkana-0.0.3-py3-none-any.whl (371 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m371.3/371.3 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: alkana\n",
            "Successfully installed alkana-0.0.3\n",
            "Collecting jaconv\n",
            "  Downloading jaconv-0.3.4.tar.gz (16 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: jaconv\n",
            "  Building wheel for jaconv (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for jaconv: filename=jaconv-0.3.4-py3-none-any.whl size=16416 sha256=161d2da9cd6632e6e3a1d84d3a44f405c35a37dabc65169888c3a62364a0bb90\n",
            "  Stored in directory: /root/.cache/pip/wheels/46/8f/2e/a730bf1fca05b33e532d5d91dabdf406c9b718ec85b01b1b54\n",
            "Successfully built jaconv\n",
            "Installing collected packages: jaconv\n",
            "Successfully installed jaconv-0.3.4\n",
            "Requirement already satisfied: librosa in /usr/local/lib/python3.10/dist-packages (0.10.1)\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.10/dist-packages (from librosa) (3.0.0)\n",
            "Requirement already satisfied: numpy!=1.22.0,!=1.22.1,!=1.22.2,>=1.20.3 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.23.5)\n",
            "Requirement already satisfied: scipy>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.11.2)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.2.2)\n",
            "Requirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.3.2)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (4.4.2)\n",
            "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.56.4)\n",
            "Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.12.1)\n",
            "Requirement already satisfied: pooch>=1.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.7.0)\n",
            "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.3.6)\n",
            "Requirement already satisfied: typing-extensions>=4.1.1 in /usr/local/lib/python3.10/dist-packages (from librosa) (4.5.0)\n",
            "Requirement already satisfied: lazy-loader>=0.1 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.3)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.0.5)\n",
            "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.51.0->librosa) (0.39.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from numba>=0.51.0->librosa) (67.7.2)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.10/dist-packages (from pooch>=1.0->librosa) (3.10.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from pooch>=1.0->librosa) (23.1)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from pooch>=1.0->librosa) (2.31.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->librosa) (3.2.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.10/dist-packages (from soundfile>=0.12.1->librosa) (1.15.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0->soundfile>=0.12.1->librosa) (2.21)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa) (2023.7.22)\n",
            "Collecting pydub\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Installing collected packages: pydub\n",
            "Successfully installed pydub-0.25.1\n",
            "Collecting pyrubberband\n",
            "  Downloading pyrubberband-0.3.0.tar.gz (4.1 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from pyrubberband) (1.16.0)\n",
            "Collecting pysoundfile>=0.8.0 (from pyrubberband)\n",
            "  Downloading PySoundFile-0.9.0.post1-py2.py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: cffi>=0.6 in /usr/local/lib/python3.10/dist-packages (from pysoundfile>=0.8.0->pyrubberband) (1.15.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=0.6->pysoundfile>=0.8.0->pyrubberband) (2.21)\n",
            "Building wheels for collected packages: pyrubberband\n",
            "  Building wheel for pyrubberband (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyrubberband: filename=pyrubberband-0.3.0-py3-none-any.whl size=4262 sha256=04a9d0762c7e5b59be39f3075b8c188528b71844efb4bf5dbdd83b89887a2beb\n",
            "  Stored in directory: /root/.cache/pip/wheels/b8/2d/f0/bb68fbfe67a42c858a79412321d28589218cbfe114c48ce664\n",
            "Successfully built pyrubberband\n",
            "Installing collected packages: pysoundfile, pyrubberband\n",
            "Successfully installed pyrubberband-0.3.0 pysoundfile-0.9.0.post1\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement stable_whisper (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for stable_whisper\u001b[0m\u001b[31m\n",
            "\u001b[0mCollecting romajitable\n",
            "  Downloading romajitable-0.0.1-py3-none-any.whl (13 kB)\n",
            "Installing collected packages: romajitable\n",
            "Successfully installed romajitable-0.0.1\n",
            "Collecting PySegmentKit\n",
            "  Downloading PySegmentKit-0.2.1-py3-none-any.whl (3.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.7/3.7 MB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PySegmentKit\n",
            "Successfully installed PySegmentKit-0.2.1\n"
          ]
        }
      ],
      "source": [
        "#import libraries\n",
        "!pip install -U git+https://github.com/jianfch/stable-ts.git\n",
        "!pip install alkana\n",
        "!pip install jaconv\n",
        "!pip install librosa\n",
        "!pip install pydub\n",
        "!pip install pyrubberband\n",
        "!pip install stable_whisper\n",
        "!pip install romajitable\n",
        "!pip install PySegmentKit"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt-get update -y\n",
        "!sudo apt-get install -y rubberband-cli"
      ],
      "metadata": {
        "id": "JveLlR8AyG75",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f6ec1b8-e6d9-4858-c735-b2275104c5e8"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rGet:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,626 B]\n",
            "\r0% [Connecting to archive.ubuntu.com (91.189.91.81)] [Connecting to security.ub\r0% [Connecting to archive.ubuntu.com (91.189.91.81)] [Connecting to security.ub\r                                                                               \rHit:2 https://ppa.launchpadcontent.net/c2d4u.team/c2d4u4.0+/ubuntu jammy InRelease\n",
            "\r0% [Connecting to archive.ubuntu.com (91.189.91.81)] [Connecting to security.ub\r                                                                               \rGet:3 http://security.ubuntu.com/ubuntu jammy-security InRelease [110 kB]\n",
            "\r0% [Waiting for headers] [3 InRelease 5,484 B/110 kB 5%] [Connected to ppa.laun\r                                                                               \rGet:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
            "\r0% [Waiting for headers] [3 InRelease 14.2 kB/110 kB 13%] [Connected to ppa.lau\r0% [Waiting for headers] [3 InRelease 14.2 kB/110 kB 13%] [Connected to ppa.lau\r0% [Waiting for headers] [Connected to ppa.launchpadcontent.net (185.125.190.52\r                                                                               \rHit:5 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "\r0% [Waiting for headers] [Connecting to ppa.launchpadcontent.net (185.125.190.5\r                                                                               \rHit:6 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:7 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Get:8 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [119 kB]\n",
            "Hit:9 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:10 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [517 kB]\n",
            "Get:11 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [998 kB]\n",
            "Get:12 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [109 kB]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,264 kB]\n",
            "Get:14 http://archive.ubuntu.com/ubuntu jammy-updates/multiverse amd64 Packages [49.8 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [1,158 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [1,284 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu jammy-backports/universe amd64 Packages [28.1 kB]\n",
            "Fetched 5,641 kB in 2s (2,873 kB/s)\n",
            "Reading package lists... Done\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following NEW packages will be installed:\n",
            "  rubberband-cli\n",
            "0 upgraded, 1 newly installed, 0 to remove and 18 not upgraded.\n",
            "Need to get 87.5 kB of archives.\n",
            "After this operation, 223 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 rubberband-cli amd64 2.0.0-2 [87.5 kB]\n",
            "Fetched 87.5 kB in 0s (198 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 1.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package rubberband-cli.\n",
            "(Reading database ... 120895 files and directories currently installed.)\n",
            "Preparing to unpack .../rubberband-cli_2.0.0-2_amd64.deb ...\n",
            "Unpacking rubberband-cli (2.0.0-2) ...\n",
            "Setting up rubberband-cli (2.0.0-2) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#hiragana to phonemes of Julius with delimiters\n",
        "def conv2julius_divide(s):\n",
        "    s = s.replace('う゛ぁ',' b a/')\n",
        "    s = s.replace('う゛ぃ',' b i/')\n",
        "    s = s.replace('う゛ぇ',' b e/')\n",
        "    s = s.replace('う゛ぉ',' b o/')\n",
        "    s = s.replace('う゛ゅ',' by u/')\n",
        "\n",
        "    s = s.replace('あぁ',' a a/')\n",
        "    s = s.replace('いぃ',' i i/')\n",
        "    s = s.replace('いぇ',' i e/')\n",
        "    s = s.replace('いゃ',' y a/')\n",
        "    s = s.replace('うぅ',' u:/')\n",
        "    s = s.replace('えぇ',' e e/')\n",
        "    s = s.replace('おぉ',' o:/')\n",
        "    s = s.replace('かぁ',' k a:/')\n",
        "    s = s.replace('きぃ',' k i:/')\n",
        "    s = s.replace('くぅ',' k u:/')\n",
        "    s = s.replace('くゃ',' ky a/')\n",
        "    s = s.replace('くゅ',' ky u/')\n",
        "    s = s.replace('くょ',' ky o/')\n",
        "    s = s.replace('けぇ',' k e:/')\n",
        "    s = s.replace('こぉ',' k o:/')\n",
        "    s = s.replace('がぁ',' g a:/')\n",
        "    s = s.replace('ぎぃ',' g i:/')\n",
        "    s = s.replace('ぐぅ',' g u:/')\n",
        "    s = s.replace('ぐゃ',' gy a/')\n",
        "    s = s.replace('ぐゅ',' gy u/')\n",
        "    s = s.replace('ぐょ',' gy o/')\n",
        "    s = s.replace('げぇ',' g e:/')\n",
        "    s = s.replace('ごぉ',' g o:/')\n",
        "    s = s.replace('さぁ',' s a:/')\n",
        "    s = s.replace('しぃ',' sh i:/')\n",
        "    s = s.replace('すぅ',' s u:/')\n",
        "    s = s.replace('すゃ',' sh a/')\n",
        "    s = s.replace('すゅ',' sh u/')\n",
        "    s = s.replace('すょ',' sh o/')\n",
        "    s = s.replace('せぇ',' s e:/')\n",
        "    s = s.replace('そぉ',' s o:/')\n",
        "    s = s.replace('ざぁ',' z a:/')\n",
        "    s = s.replace('じぃ',' j i:/')\n",
        "    s = s.replace('ずぅ',' z u:/')\n",
        "    s = s.replace('ずゃ',' zy a/')\n",
        "    s = s.replace('ずゅ',' zy u/')\n",
        "    s = s.replace('ずょ',' zy o/')\n",
        "    s = s.replace('ぜぇ',' z e:/')\n",
        "    s = s.replace('ぞぉ',' z o:/')\n",
        "    s = s.replace('たぁ',' t a:/')\n",
        "    s = s.replace('ちぃ',' ch i:/')\n",
        "    s = s.replace('つぁ',' ts a/')\n",
        "    s = s.replace('つぃ',' ts i/')\n",
        "    s = s.replace('つぅ',' ts u:/')\n",
        "    s = s.replace('つゃ',' ch a/')\n",
        "    s = s.replace('つゅ',' ch u/')\n",
        "    s = s.replace('つょ',' ch o/')\n",
        "    s = s.replace('つぇ',' ts e/')\n",
        "    s = s.replace('つぉ',' ts o/')\n",
        "    s = s.replace('てぇ',' t e:/')\n",
        "    s = s.replace('とぉ',' t o:/')\n",
        "    s = s.replace('だぁ',' d a:/')\n",
        "    s = s.replace('ぢぃ',' j i:/')\n",
        "    s = s.replace('づぅ',' d u:/')\n",
        "    s = s.replace('づゃ',' zy a/')\n",
        "    s = s.replace('づゅ',' zy u/')\n",
        "    s = s.replace('づょ',' zy o/')\n",
        "    s = s.replace('でぇ',' d e:/')\n",
        "    s = s.replace('どぉ',' d o:/')\n",
        "    s = s.replace('なぁ',' n a:/')\n",
        "    s = s.replace('にぃ',' n i:/')\n",
        "    s = s.replace('ぬぅ',' n u:/')\n",
        "    s = s.replace('ぬゃ',' ny a/')\n",
        "    s = s.replace('ぬゅ',' ny u/')\n",
        "    s = s.replace('ぬょ',' ny o/')\n",
        "    s = s.replace('ねぇ',' n e:/')\n",
        "    s = s.replace('のぉ',' n o:/')\n",
        "    s = s.replace('はぁ',' h a:/')\n",
        "    s = s.replace('ひぃ',' h i:/')\n",
        "    s = s.replace('ふぅ',' f u:/')\n",
        "    s = s.replace('ふゃ',' hy a/')\n",
        "    s = s.replace('ふゅ',' hy u/')\n",
        "    s = s.replace('ふょ',' hy o/')\n",
        "    s = s.replace('へぇ',' h e:/')\n",
        "    s = s.replace('ほぉ',' h o:/')\n",
        "    s = s.replace('ばぁ',' b a:/')\n",
        "    s = s.replace('びぃ',' b i:/')\n",
        "    s = s.replace('ぶぅ',' b u:/')\n",
        "    s = s.replace('ふゃ',' hy a/')\n",
        "    s = s.replace('ぶゅ',' by u/')\n",
        "    s = s.replace('ふょ',' hy o/')\n",
        "    s = s.replace('べぇ',' b e:/')\n",
        "    s = s.replace('ぼぉ',' b o:/')\n",
        "    s = s.replace('ぱぁ',' p a:/')\n",
        "    s = s.replace('ぴぃ',' p i:/')\n",
        "    s = s.replace('ぷぅ',' p u:/')\n",
        "    s = s.replace('ぷゃ',' py a/')\n",
        "    s = s.replace('ぷゅ',' py u/')\n",
        "    s = s.replace('ぷょ',' py o/')\n",
        "    s = s.replace('ぺぇ',' p e:/')\n",
        "    s = s.replace('ぽぉ',' p o:/')\n",
        "    s = s.replace('まぁ',' m a:/')\n",
        "    s = s.replace('みぃ',' m i:/')\n",
        "    s = s.replace('むぅ',' m u:/')\n",
        "    s = s.replace('むゃ',' my a/')\n",
        "    s = s.replace('むゅ',' my u/')\n",
        "    s = s.replace('むょ',' my o/')\n",
        "    s = s.replace('めぇ',' m e:/')\n",
        "    s = s.replace('もぉ',' m o:/')\n",
        "    s = s.replace('やぁ',' y a:/')\n",
        "    s = s.replace('ゆぅ',' y u:/')\n",
        "    s = s.replace('ゆゃ',' y a:/')\n",
        "    s = s.replace('ゆゅ',' y u:/')\n",
        "    s = s.replace('ゆょ',' y o:/')\n",
        "    s = s.replace('よぉ',' y o:/')\n",
        "    s = s.replace('らぁ',' r a:/')\n",
        "    s = s.replace('りぃ',' r i:/')\n",
        "    s = s.replace('るぅ',' r u:/')\n",
        "    s = s.replace('るゃ',' ry a/')\n",
        "    s = s.replace('るゅ',' ry u/')\n",
        "    s = s.replace('るょ',' ry o/')\n",
        "    s = s.replace('れぇ',' r e:/')\n",
        "    s = s.replace('ろぉ',' r o:/')\n",
        "    s = s.replace('わぁ',' w a:/')\n",
        "    s = s.replace('をぉ',' o:/')\n",
        "\n",
        "    s = s.replace('う゛',' b u/')\n",
        "    s = s.replace('でぃ',' d i/')\n",
        "    s = s.replace('でぇ',' d e:/')\n",
        "    s = s.replace('でゃ',' dy a/')\n",
        "    s = s.replace('でゅ',' dy u/')\n",
        "    s = s.replace('でょ',' dy o/')\n",
        "    s = s.replace('てぃ',' t i/')\n",
        "    s = s.replace('てぇ',' t e:/')\n",
        "    s = s.replace('てゃ',' ty a/')\n",
        "    s = s.replace('てゅ',' ty u/')\n",
        "    s = s.replace('てょ',' ty o/')\n",
        "    s = s.replace('すぃ',' s i/')\n",
        "    s = s.replace('ずぁ',' z u a/')\n",
        "    s = s.replace('ずぃ',' z i/')\n",
        "    s = s.replace('ずぅ',' z u/')\n",
        "    s = s.replace('ずゃ',' zy a/')\n",
        "    s = s.replace('ずゅ',' zy u/')\n",
        "    s = s.replace('ずょ',' zy o/')\n",
        "    s = s.replace('ずぇ',' z e/')\n",
        "    s = s.replace('ずぉ',' z o/')\n",
        "    s = s.replace('きゃ',' ky a/')\n",
        "    s = s.replace('きゅ',' ky u/')\n",
        "    s = s.replace('きょ',' ky o/')\n",
        "    s = s.replace('しゃ',' sh a/')\n",
        "    s = s.replace('しゅ',' sh u/')\n",
        "    s = s.replace('しぇ',' sh e/')\n",
        "    s = s.replace('しょ',' sh o/')\n",
        "    s = s.replace('ちゃ',' ch a/')\n",
        "    s = s.replace('ちゅ',' ch u/')\n",
        "    s = s.replace('ちぇ',' ch e/')\n",
        "    s = s.replace('ちょ',' ch o/')\n",
        "    s = s.replace('とぅ',' t u/')\n",
        "    s = s.replace('とゃ',' ty a/')\n",
        "    s = s.replace('とゅ',' ty u/')\n",
        "    s = s.replace('とょ',' ty o/')\n",
        "    s = s.replace('どぁ',' d o a/')\n",
        "    s = s.replace('どぅ',' d u/')\n",
        "    s = s.replace('どゃ',' dy a/')\n",
        "    s = s.replace('どゅ',' dy u/')\n",
        "    s = s.replace('どょ',' dy o/')\n",
        "    s = s.replace('どぉ',' d o:/')\n",
        "    s = s.replace('にゃ',' ny a/')\n",
        "    s = s.replace('にゅ',' ny u/')\n",
        "    s = s.replace('にょ',' ny o/')\n",
        "    s = s.replace('ひゃ',' hy a/')\n",
        "    s = s.replace('ひゅ',' hy u/')\n",
        "    s = s.replace('ひょ',' hy o/')\n",
        "    s = s.replace('みゃ',' my a/')\n",
        "    s = s.replace('みゅ',' my u/')\n",
        "    s = s.replace('みょ',' my o/')\n",
        "    s = s.replace('りゃ',' ry a/')\n",
        "    s = s.replace('りゅ',' ry u/')\n",
        "    s = s.replace('りょ',' ry o/')\n",
        "    s = s.replace('ぎゃ',' gy a/')\n",
        "    s = s.replace('ぎゅ',' gy u/')\n",
        "    s = s.replace('ぎょ',' gy o/')\n",
        "    s = s.replace('ぢぇ',' j e/')\n",
        "    s = s.replace('ぢゃ',' j a/')\n",
        "    s = s.replace('ぢゅ',' j u/')\n",
        "    s = s.replace('ぢょ',' j o/')\n",
        "    s = s.replace('じぇ',' j e/')\n",
        "    s = s.replace('じゃ',' j a/')\n",
        "    s = s.replace('じゅ',' j u/')\n",
        "    s = s.replace('じょ',' j o/')\n",
        "    s = s.replace('びゃ',' by a/')\n",
        "    s = s.replace('びゅ',' by u/')\n",
        "    s = s.replace('びょ',' by o/')\n",
        "    s = s.replace('ぴゃ',' py a/')\n",
        "    s = s.replace('ぴゅ',' py u/')\n",
        "    s = s.replace('ぴょ',' py o/')\n",
        "    s = s.replace('うぁ',' u a/')\n",
        "    s = s.replace('うぃ',' w i/')\n",
        "    s = s.replace('うぇ',' w e/')\n",
        "    s = s.replace('うぉ',' w o/')\n",
        "    s = s.replace('ふぁ',' f a/')\n",
        "    s = s.replace('ふぃ',' f i/')\n",
        "    s = s.replace('ふぅ',' f u/')\n",
        "    s = s.replace('ふゃ',' hy a/')\n",
        "    s = s.replace('ふゅ',' hy u/')\n",
        "    s = s.replace('ふょ',' hy o/')\n",
        "    s = s.replace('ふぇ',' f e/')\n",
        "    s = s.replace('ふぉ',' f o/')\n",
        "\n",
        "    s = s.replace('あ',' a/')\n",
        "    s = s.replace('い',' i/')\n",
        "    s = s.replace('う',' u/')\n",
        "    s = s.replace('え',' e/')\n",
        "    s = s.replace('お',' o/')\n",
        "    s = s.replace('か',' k a/')\n",
        "    s = s.replace('き',' k i/')\n",
        "    s = s.replace('く',' k u/')\n",
        "    s = s.replace('け',' k e/')\n",
        "    s = s.replace('こ',' k o/')\n",
        "    s = s.replace('さ',' s a/')\n",
        "    s = s.replace('し',' sh i/')\n",
        "    s = s.replace('す',' s u/')\n",
        "    s = s.replace('せ',' s e/')\n",
        "    s = s.replace('そ',' s o/')\n",
        "    s = s.replace('た',' t a/')\n",
        "    s = s.replace('ち',' ch i/')\n",
        "    s = s.replace('つ',' ts u/')\n",
        "    s = s.replace('て',' t e/')\n",
        "    s = s.replace('と',' t o/')\n",
        "    s = s.replace('な',' n a/')\n",
        "    s = s.replace('に',' n i/')\n",
        "    s = s.replace('ぬ',' n u/')\n",
        "    s = s.replace('ね',' n e/')\n",
        "    s = s.replace('の',' n o/')\n",
        "    s = s.replace('は',' h a/')\n",
        "    s = s.replace('ひ',' h i/')\n",
        "    s = s.replace('ふ',' f u/')\n",
        "    s = s.replace('へ',' h e/')\n",
        "    s = s.replace('ほ',' h o/')\n",
        "    s = s.replace('ま',' m a/')\n",
        "    s = s.replace('み',' m i/')\n",
        "    s = s.replace('む',' m u/')\n",
        "    s = s.replace('め',' m e/')\n",
        "    s = s.replace('も',' m o/')\n",
        "    s = s.replace('ら',' r a/')\n",
        "    s = s.replace('り',' r i/')\n",
        "    s = s.replace('る',' r u/')\n",
        "    s = s.replace('れ',' r e/')\n",
        "    s = s.replace('ろ',' r o/')\n",
        "    s = s.replace('が',' g a/')\n",
        "    s = s.replace('ぎ',' g i/')\n",
        "    s = s.replace('ぐ',' g u/')\n",
        "    s = s.replace('げ',' g e/')\n",
        "    s = s.replace('ご',' g o/')\n",
        "    s = s.replace('ざ',' z a/')\n",
        "    s = s.replace('じ',' j i/')\n",
        "    s = s.replace('ず',' z u/')\n",
        "    s = s.replace('ぜ',' z e/')\n",
        "    s = s.replace('ぞ',' z o/')\n",
        "    s = s.replace('だ',' d a/')\n",
        "    s = s.replace('ぢ',' j i/')\n",
        "    s = s.replace('づ',' z u/')\n",
        "    s = s.replace('で',' d e/')\n",
        "    s = s.replace('ど',' d o/')\n",
        "    s = s.replace('ば',' b a/')\n",
        "    s = s.replace('び',' b i/')\n",
        "    s = s.replace('ぶ',' b u/')\n",
        "    s = s.replace('べ',' b e/')\n",
        "    s = s.replace('ぼ',' b o/')\n",
        "    s = s.replace('ぱ',' p a/')\n",
        "    s = s.replace('ぴ',' p i/')\n",
        "    s = s.replace('ぷ',' p u/')\n",
        "    s = s.replace('ぺ',' p e/')\n",
        "    s = s.replace('ぽ',' p o/')\n",
        "    s = s.replace('や',' y a/')\n",
        "    s = s.replace('ゆ',' y u/')\n",
        "    s = s.replace('よ',' y o/')\n",
        "    s = s.replace('わ',' w a/')\n",
        "    s = s.replace('を',' o/')\n",
        "    s = s.replace('ん',' N/')\n",
        "    s = s.replace('っ',' q/')\n",
        "    if ('ー' in s):\n",
        "      s = s.replace(\"/ー\",\"ー\")\n",
        "      s = s.replace('ー',':/')\n",
        "\n",
        "    s = s.replace('ぁ',' a/')\n",
        "    s = s.replace('ぃ',' i/')\n",
        "    s = s.replace('ぅ',' u/')\n",
        "    s = s.replace('ぇ',' e/')\n",
        "    s = s.replace('ぉ',' o/')\n",
        "    s = s.replace('ゎ',' w a/')\n",
        "\n",
        "    s = s[1:]\n",
        "\n",
        "    s = re.sub(r':+', ':', s)\n",
        "\n",
        "    return s"
      ],
      "metadata": {
        "id": "s5g5Qgb86NNx"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import glob\n",
        "import stable_whisper\n",
        "from collections import OrderedDict\n",
        "import re\n",
        "import jaconv\n",
        "import alkana\n",
        "import librosa\n",
        "import time\n",
        "import soundfile as sf\n",
        "import shutil\n",
        "import pathlib\n",
        "from pydub import AudioSegment\n",
        "from pydub.silence import split_on_silence\n",
        "import wave\n",
        "import pyrubberband as pyrb\n",
        "from natsort import natsorted\n",
        "import romajitable\n",
        "from PySegmentKit import PySegmentKit, PSKError\n",
        "import alkana\n",
        "import re\n",
        "\n",
        "#calculating audio duration\n",
        "def get_audio_duration(file_path):\n",
        "  with wave.open(file_path,  'rb') as wr:\n",
        "    fn = wr.getnframes()\n",
        "    fr = wr.getframerate()\n",
        "    duration =  1.0 * fn / fr\n",
        "    return duration\n",
        "\n",
        "#adjusting duration of last words on julius .lab\n",
        "def adjust_max_sec(od,file_name):\n",
        "  data = od.popitem()\n",
        "  with wave.open(file_name,  'rb') as wr:\n",
        "      fr = wr.getframerate()\n",
        "      fn = wr.getnframes()\n",
        "      length = 1.0*fn/fr\n",
        "\n",
        "  od[data[0].replace(\".\",\"\")] = [data[1][0],length]\n",
        "  od.move_to_end(data[0].replace(\".\",\"\"))\n",
        "  print(od[data[0].replace(\".\",\"\")])\n",
        "\n",
        "  return od\n",
        "\n",
        "#speech recogniton using whisper & making phoneme dictionary\n",
        "def load_recognize_speech(file_name):\n",
        "  #load & recognition\n",
        "  files:list[str] = [file_name]\n",
        "  od = OrderedDict()\n",
        "  new_od = OrderedDict()\n",
        "  model = stable_whisper.load_model('base')\n",
        "  recoginzed_txt = \"\"\n",
        "  for i, file in enumerate(files):\n",
        "    print(\"## {}\".format(file))\n",
        "    result = model.transcribe(file, language= \"en\", verbose=True)\n",
        "    for word in result.all_words():\n",
        "\n",
        "      if(word.word[1:] in od):\n",
        "        od[word.word[1:]].append(word.start)\n",
        "        od[word.word[1:]].append(word.end)\n",
        "      else:\n",
        "        od[word.word[1:]] = [word.start, word.end]\n",
        "\n",
        "      recoginzed_txt = recoginzed_txt + word.word + \" \"\n",
        "\n",
        "  new_od = adjust_max_sec(od,file_name)\n",
        "  first_key = next(iter(new_od))\n",
        "  new_od[first_key] = [0.0,new_od[first_key][1]]\n",
        "  recoginzed_txt = recoginzed_txt[:-1]\n",
        "\n",
        "  return new_od,recoginzed_txt\n",
        "\n",
        "#converting from English to Japanese hiragana for a sentence\n",
        "def converter_en_to_jp_sen(txt):\n",
        "  txt_split = txt[:-1].split(\" \")\n",
        "  txt_split = list(filter(lambda x: x != \"\", txt_split))\n",
        "  katakana_txt_list = []\n",
        "  for i in txt_split:\n",
        "    if(alkana.get_kana(i) is None):#if there are no information in the dictionary, use rulebase\n",
        "      katakana_txt_list.append(romajitable.to_kana(i).katakana)\n",
        "    else:\n",
        "      katakana_txt_list.append(alkana.get_kana(i))\n",
        "\n",
        "  katakana_txt = \" \".join(katakana_txt_list)\n",
        "  kana_txt =  jaconv.kata2hira(katakana_txt)\n",
        "  kana_txt = re.sub(r'[^ぁ-んァ-ンー]', '',kana_txt)\n",
        "  kana_txt = kana_txt.replace(\"ゔ\",\"う゛\")\n",
        "  katakana_pho = jaconv.hiragana2julius(kana_txt)\n",
        "\n",
        "\n",
        "  return katakana_pho,kana_txt.replace(\" \",\"\")\n",
        "\n",
        "#converting from English to Japanese hiragana for a word\n",
        "def converter_en_to_jp_word(word):\n",
        "  if(alkana.get_kana(word) is None):\n",
        "    word =  romajitable.to_kana(word)\n",
        "    word = word.katakana\n",
        "  else:\n",
        "    word = alkana.get_kana(word)\n",
        "\n",
        "  word =  jaconv.kata2hira(word)\n",
        "  word = word.replace(\"ゔ\",\"う゛\")\n",
        "  word = re.sub(r'[^ぁ-んァ-ンー]', '',word)\n",
        "  word = conv2julius_divide(word)\n",
        "\n",
        "  return word\n",
        "\n",
        "#resampling for audio\n",
        "def resample_wav(input_path, output_path, target_sr=16000):\n",
        "    y, sr = librosa.load(input_path, sr=None)\n",
        "    y_resampled = librosa.resample(y, orig_sr=sr, target_sr=target_sr)\n",
        "    sf.write(output_path, y_resampled, target_sr, subtype=\"PCM_16\")\n",
        "\n",
        "#making dictionary from results of Julius\n",
        "def julius_analysis(lab_path):\n",
        "  data_dict = {}\n",
        "  with open(lab_path, \"r\") as file:\n",
        "            print(file)\n",
        "            for line in file:\n",
        "              start_time, end_time, phone = line.strip().split(' ')\n",
        "              if(phone in data_dict):\n",
        "                data_dict[phone] += [start_time,end_time]\n",
        "              else:\n",
        "                data_dict[phone] = [start_time,end_time]\n",
        "  return data_dict\n",
        "\n",
        "#conducting phoneme segmentation using Julius\n",
        "def julius(dir):\n",
        "  sk = PySegmentKit(dir,\n",
        "    disable_silence_at_ends=False,\n",
        "    leave_dict=False,\n",
        "    debug=False,\n",
        "    triphone=False,\n",
        "    input_mfcc=False)\n",
        "\n",
        "  try:\n",
        "      segmented = sk.segment()\n",
        "      print(\"julius-results\")\n",
        "      for result in segmented.keys():\n",
        "          print(\"=====Segmentation result of {}.wav=====\".format(result))\n",
        "          for begintime, endtime, unit in segmented[result]:\n",
        "              print(\"{:.7f} {:.7f} {}\".format(begintime, endtime, unit))\n",
        "\n",
        "  except PSKError as e:\n",
        "      print(e)\n",
        "\n",
        "\n",
        "#Making dictionary from Japanese phoneme & English phoneme\n",
        "def make_duration_jp(od_speech_a,od_speech_b):\n",
        "  time_list = []\n",
        "  new_od_speech_b = OrderedDict()\n",
        "  for i in od_speech_a.keys():\n",
        "    search_sentence_list = i.split(\"/\")\n",
        "    for phone in search_sentence_list:\n",
        "      search_phone_list = phone.split(\" \")\n",
        "      search_phone_list = list(filter(lambda x: x != \"\", search_phone_list))\n",
        "\n",
        "      for j in search_phone_list:\n",
        "        time_list += [float(od_speech_b[j][0]),float(od_speech_b[j][1])]\n",
        "        del od_speech_b[j][0:2]\n",
        "\n",
        "      min_data = min(time_list)\n",
        "      max_data = max(time_list)\n",
        "\n",
        "    new_od_speech_b[i] = [min_data,max_data]\n",
        "    time_list.clear()\n",
        "\n",
        "  first_key = next(iter(new_od_speech_b))\n",
        "  new_od_speech_b[first_key] = [float(od_speech_b[\"silB\"][0]),new_od_speech_b[first_key][1]]\n",
        "  last_key = next(reversed(new_od_speech_b))\n",
        "  new_od_speech_b[last_key] = [new_od_speech_b[last_key][0],float(od_speech_b[\"silE\"][1])]\n",
        "  return new_od_speech_b\n",
        "\n",
        "#Making audio files for each word\n",
        "def split_wav_usedict(input_file, output_folder, dict):\n",
        "    if not os.path.exists(output_folder):\n",
        "        os.makedirs(output_folder)\n",
        "    num = 0\n",
        "    for i in dict:\n",
        "      sourceAudio = AudioSegment.from_wav(input_file)\n",
        "      processedAudio =  sourceAudio[dict[i][0]*1000:dict[i][1]*1000]\n",
        "      processedAudio.export(output_folder + \"/\"+str(num)+\".wav\", format=\"wav\")\n",
        "      num += 1\n",
        "\n",
        "\n",
        "#Adjusting Japanese speech using English duration\n",
        "def align_speech(file_b,od_speech_a,od_speech_b):#発話区間調整関数\n",
        "  field_put_folder = \"/content/field\"\n",
        "  out_put_folder = \"/content/output\"\n",
        "\n",
        "  if not os.path.exists(out_put_folder):\n",
        "    os.makedirs(out_put_folder)\n",
        "\n",
        "  split_wav_usedict(file_b,field_put_folder,od_speech_b)\n",
        "  count = 0\n",
        "\n",
        "  for i in od_speech_b.keys():\n",
        "    file_path_a = field_put_folder + \"/\" + str(count) + \".wav\"\n",
        "    data_one = od_speech_a[i][1]\n",
        "    data_zero = od_speech_a[i][0]\n",
        "    target_duration_a =  data_one - data_zero\n",
        "    source_duration = get_audio_duration(file_path_a)\n",
        "    multi_num = source_duration/target_duration_a\n",
        "    print(multi_num)\n",
        "    y, sr   = librosa.load(file_path_a, sr=16000, mono=True)\n",
        "    y_fast = pyrb.time_stretch(y, sr, multi_num)\n",
        "    sf.write(file_path_a,y_fast,sr)\n",
        "\n",
        "    count += 1\n",
        "\n",
        "  for k in range(count):\n",
        "    if(k == 0):\n",
        "      audio = AudioSegment.from_wav( field_put_folder + \"/\" + str(k) + \".wav\")\n",
        "    else:\n",
        "      audio+=AudioSegment.from_wav( field_put_folder + \"/\" + str(k) + \".wav\")\n",
        "\n",
        "  shutil.rmtree( field_put_folder)\n",
        "  os.mkdir( field_put_folder)\n",
        "\n",
        "  audio.export(out_put_folder + \"/\" + str(file_b).split(\"/\")[-1], format=\"wav\")\n",
        "\n",
        "#Making a directory concisted of audio & text\n",
        "def make_dir(audio_path,txt_path):\n",
        "    output_directory = \"/content/path_to_output_directory\"\n",
        "    os.makedirs(output_directory, exist_ok=True)\n",
        "    shutil.copy(audio_path, os.path.join(output_directory, \"tem.wav\"))\n",
        "    shutil.copy(txt_path, os.path.join(output_directory, \"tem.txt\"))\n",
        "\n",
        "    return output_directory\n",
        "\n",
        "#Adjusting seconds in dictionaries\n",
        "def adjust_dict(dict_tem):\n",
        "  stack =  OrderedDict()\n",
        "  for key, value in dict_tem.items():\n",
        "    if not stack:\n",
        "      stack[key] = value\n",
        "    else:\n",
        "      first_key = next(iter(stack))\n",
        "      if not stack[first_key][1] == value[0]:\n",
        "        dict_tem[key] = [stack[first_key][1],value[1]]\n",
        "        stack.clear()\n",
        "      else:\n",
        "        stack.clear()\n",
        "  return dict_tem"
      ],
      "metadata": {
        "id": "xKLyNjpySQHp"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#main\n",
        "def main():\n",
        "    folder_path_us = \"/content/us\"\n",
        "    folder_path_ja = \"/content/ja\"\n",
        "    tem_path_txt_file = \"/content/tem.txt\"\n",
        "    for filename_a in natsorted(os.listdir(folder_path_us)):\n",
        "        file_path_us = os.path.join(folder_path_us, filename_a)\n",
        "        file_path_ja = os.path.join(folder_path_ja, filename_a)\n",
        "\n",
        "        model = stable_whisper.load_model('base')\n",
        "\n",
        "        od_speech_us = OrderedDict()\n",
        "        od_speech_us,recognized_text_a = load_recognize_speech(file_path_us)\n",
        "\n",
        "        new_od_speech_us = OrderedDict()\n",
        "        for i in od_speech_us.keys():\n",
        "            new_key = converter_en_to_jp_word(i)\n",
        "            new_od_speech_us[new_key] = od_speech_us[i]\n",
        "\n",
        "\n",
        "\n",
        "        japanaized_phone,kana_txt = converter_en_to_jp_sen(recognized_text_a)\n",
        "\n",
        "        f = open(tem_path_txt_file, 'w', encoding=\"utf-8\")\n",
        "        f.write(kana_txt)\n",
        "        f.close()\n",
        "\n",
        "        resample_wav(file_path_ja, file_path_ja, target_sr=16000)\n",
        "        output = make_dir(file_path_ja, tem_path_txt_file)\n",
        "        julius(output)\n",
        "        od_speech_ja = julius_analysis(output + \"/tem.lab\")\n",
        "        shutil.rmtree(output)\n",
        "\n",
        "        new_od_speech_ja = make_duration_jp(new_od_speech_us,od_speech_ja)\n",
        "        new_od_speech_us = adjust_dict(new_od_speech_us)\n",
        "        new_od_speech_ja = adjust_dict(new_od_speech_ja)\n",
        "\n",
        "\n",
        "        print(new_od_speech_us)\n",
        "        print(new_od_speech_ja)\n",
        "\n",
        "        align_speech(file_path_ja,new_od_speech_us,new_od_speech_ja)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZU0hXXz2GvFu",
        "outputId": "7da748c5-84d2-400e-8fe1-6c7a985d58fe"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "## /content/us/arctic_a0001.wav\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/stable_whisper/whisper_word_level.py:235: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[00:00.060 --> 00:02.920]  Author of the Danger Trail, Philip Steele's, etc.\n",
            "-[00:00.060] -> [00:00.320] \" Author\"\n",
            "-[00:00.320] -> [00:00.520] \" of\"\n",
            "-[00:00.520] -> [00:00.600] \" the\"\n",
            "-[00:00.600] -> [00:00.840] \" Danger\"\n",
            "-[00:00.840] -> [00:01.240] \" Trail,\"\n",
            "-[00:01.660] -> [00:01.840] \" Philip\"\n",
            "-[00:01.840] -> [00:02.380] \" Steele's,\"\n",
            "-[00:02.640] -> [00:02.920] \" etc.\"\n",
            "\n",
            "[2.64, 3.288]\n",
            "/content/path_to_output_directory/tem.wav\n",
            "Result saved in \"/content/path_to_output_directory/tem.lab\".\n",
            "\n",
            "julius-results\n",
            "=====Segmentation result of /content/path_to_output_directory/tem.wav=====\n",
            "0.0000000 0.1225000 silB\n",
            "0.1225000 0.3025000 o:\n",
            "0.3025000 0.4525000 s\n",
            "0.4525000 0.7325000 a:\n",
            "0.7325000 0.7625000 o\n",
            "0.7625000 0.8125000 b\n",
            "0.8125000 0.8825000 u\n",
            "0.8825000 0.9925000 z\n",
            "0.9925000 1.0825000 a\n",
            "1.0825000 1.1625000 d\n",
            "1.1625000 1.2425000 e\n",
            "1.2425000 1.2725000 i\n",
            "1.2725000 1.3425000 N\n",
            "1.3425000 1.4225000 j\n",
            "1.4225000 1.5625000 a:\n",
            "1.5625000 1.6725000 t\n",
            "1.6725000 1.7325000 o\n",
            "1.7325000 1.8825000 r\n",
            "1.8825000 1.9125000 a\n",
            "1.9125000 1.9425000 i\n",
            "1.9425000 1.9825000 r\n",
            "1.9825000 2.0925000 u\n",
            "2.0925000 2.4225000 f\n",
            "2.4225000 2.4725000 i\n",
            "2.4725000 2.5025000 r\n",
            "2.5025000 2.6325000 i\n",
            "2.6325000 2.7025000 q\n",
            "2.7025000 2.7525000 p\n",
            "2.7525000 2.7825000 u\n",
            "2.7825000 2.9025000 s\n",
            "2.9025000 2.9325000 u\n",
            "2.9325000 2.9925000 t\n",
            "2.9925000 3.0425000 e:\n",
            "3.0425000 3.0725000 r\n",
            "3.0725000 3.1325000 e\n",
            "3.1325000 3.2625000 s\n",
            "3.2625000 3.3725000 u\n",
            "3.3725000 3.4025000 i:\n",
            "3.4025000 3.7725000 t\n",
            "3.7725000 3.8025000 i:\n",
            "3.8025000 3.9325000 sh\n",
            "3.9325000 4.0125000 i:\n",
            "4.0125000 4.2725000 silE\n",
            "<_io.TextIOWrapper name='/content/path_to_output_directory/tem.lab' mode='r' encoding='UTF-8'>\n",
            "OrderedDict([('o:/ s a:/', [0.0, 0.32]), ('o/ b u/', [0.32, 0.52]), ('z a/', [0.52, 0.6]), ('d e/ i/ N/ j a:/', [0.6, 0.84]), ('t o/ r a/ i/ r u/', [0.84, 1.24]), ('f i/ r i/ q/ p u/', [1.24, 1.84]), ('s u/ t e:/ r e/ s u/', [1.84, 2.38]), ('i:/ t i:/ sh i:/', [2.38, 3.288])])\n",
            "OrderedDict([('o:/ s a:/', [0.0, 0.7325]), ('o/ b u/', [0.7325, 0.8825]), ('z a/', [0.8825, 1.0825]), ('d e/ i/ N/ j a:/', [1.0825, 1.5625]), ('t o/ r a/ i/ r u/', [1.5625, 2.0925]), ('f i/ r i/ q/ p u/', [2.0925, 2.7825]), ('s u/ t e:/ r e/ s u/', [2.7825, 3.3725]), ('i:/ t i:/ sh i:/', [3.3725, 4.2725])])\n",
            "2.2890625\n",
            "0.7499999999999999\n",
            "2.5000000000000013\n",
            "2.0\n",
            "1.325\n",
            "1.1499999999999997\n",
            "1.092592592592593\n",
            "0.9911894273127755\n"
          ]
        }
      ]
    }
  ]
}