{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **jaeeadjuster**\n",
        "This is a forced alignment tool for adjusting duration of words between Native speaker's English & Japanese-accented English.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZMdK_6xXYTUm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z6IydpCEEYLq",
        "outputId": "ab517bc3-c8a4-42f1-9732-b542bfdffdbc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/jianfch/stable-ts.git\n",
            "  Cloning https://github.com/jianfch/stable-ts.git to /tmp/pip-req-build-_l0tad_g\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/jianfch/stable-ts.git /tmp/pip-req-build-_l0tad_g\n",
            "  Resolved https://github.com/jianfch/stable-ts.git to commit 71b9f1fcbd1268f8bfe95bba6a394a2bc2e7339b\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from stable-ts==2.13.4) (1.23.5)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from stable-ts==2.13.4) (2.1.0+cu118)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.10/dist-packages (from stable-ts==2.13.4) (2.1.0+cu118)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from stable-ts==2.13.4) (4.66.1)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.10/dist-packages (from stable-ts==2.13.4) (10.1.0)\n",
            "Requirement already satisfied: transformers>=4.19.0 in /usr/local/lib/python3.10/dist-packages (from stable-ts==2.13.4) (4.35.2)\n",
            "Collecting ffmpeg-python==0.2.0 (from stable-ts==2.13.4)\n",
            "  Downloading ffmpeg_python-0.2.0-py3-none-any.whl (25 kB)\n",
            "Collecting openai-whisper==20231117 (from stable-ts==2.13.4)\n",
            "  Downloading openai-whisper-20231117.tar.gz (798 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m798.6/798.6 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.10/dist-packages (from ffmpeg-python==0.2.0->stable-ts==2.13.4) (0.18.3)\n",
            "Requirement already satisfied: triton<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117->stable-ts==2.13.4) (2.1.0)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117->stable-ts==2.13.4) (0.58.1)\n",
            "Collecting tiktoken (from openai-whisper==20231117->stable-ts==2.13.4)\n",
            "  Downloading tiktoken-0.5.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m73.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers>=4.19.0->stable-ts==2.13.4) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.19.0->stable-ts==2.13.4) (0.19.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.19.0->stable-ts==2.13.4) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.19.0->stable-ts==2.13.4) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.19.0->stable-ts==2.13.4) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers>=4.19.0->stable-ts==2.13.4) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.19.0->stable-ts==2.13.4) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.19.0->stable-ts==2.13.4) (0.4.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->stable-ts==2.13.4) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->stable-ts==2.13.4) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->stable-ts==2.13.4) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->stable-ts==2.13.4) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->stable-ts==2.13.4) (2023.6.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->stable-ts==2.13.4) (2.1.3)\n",
            "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->openai-whisper==20231117->stable-ts==2.13.4) (0.41.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.19.0->stable-ts==2.13.4) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.19.0->stable-ts==2.13.4) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.19.0->stable-ts==2.13.4) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.19.0->stable-ts==2.13.4) (2023.7.22)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->stable-ts==2.13.4) (1.3.0)\n",
            "Building wheels for collected packages: stable-ts, openai-whisper\n",
            "  Building wheel for stable-ts (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for stable-ts: filename=stable_ts-2.13.4-py3-none-any.whl size=77579 sha256=7e4047f268f5cf7f1e4542741cfb6f4a26eb6f66f24f9f6a93f1f0aeae9f940d\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-g6ae3r0g/wheels/5a/48/64/a463d57ac05105e1692e3649ca76cea98a8867262d7b32dd86\n",
            "  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for openai-whisper: filename=openai_whisper-20231117-py3-none-any.whl size=801358 sha256=6b47b9d4481892ccf67df4ce6078941be45172e4f28f6054eb96ed9cd1f9b986\n",
            "  Stored in directory: /root/.cache/pip/wheels/d0/85/e1/9361b4cbea7dd4b7f6702fa4c3afc94877952eeb2b62f45f56\n",
            "Successfully built stable-ts openai-whisper\n",
            "Installing collected packages: ffmpeg-python, tiktoken, openai-whisper, stable-ts\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\n",
            "llmx 0.0.15a0 requires openai, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed ffmpeg-python-0.2.0 openai-whisper-20231117 stable-ts-2.13.4 tiktoken-0.5.1\n",
            "Collecting alkana\n",
            "  Downloading alkana-0.0.3-py3-none-any.whl (371 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m371.3/371.3 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: alkana\n",
            "Successfully installed alkana-0.0.3\n",
            "Collecting jaconv\n",
            "  Downloading jaconv-0.3.4.tar.gz (16 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: jaconv\n",
            "  Building wheel for jaconv (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for jaconv: filename=jaconv-0.3.4-py3-none-any.whl size=16415 sha256=d73d64c9aa570c4ecdb5ca4c1efb7ef5406137dd62926bf57347d9d503aeaf9a\n",
            "  Stored in directory: /root/.cache/pip/wheels/46/8f/2e/a730bf1fca05b33e532d5d91dabdf406c9b718ec85b01b1b54\n",
            "Successfully built jaconv\n",
            "Installing collected packages: jaconv\n",
            "Successfully installed jaconv-0.3.4\n",
            "Requirement already satisfied: librosa in /usr/local/lib/python3.10/dist-packages (0.10.1)\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.10/dist-packages (from librosa) (3.0.1)\n",
            "Requirement already satisfied: numpy!=1.22.0,!=1.22.1,!=1.22.2,>=1.20.3 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.23.5)\n",
            "Requirement already satisfied: scipy>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.11.3)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.2.2)\n",
            "Requirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.3.2)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (4.4.2)\n",
            "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.58.1)\n",
            "Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.12.1)\n",
            "Requirement already satisfied: pooch>=1.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.8.0)\n",
            "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.3.7)\n",
            "Requirement already satisfied: typing-extensions>=4.1.1 in /usr/local/lib/python3.10/dist-packages (from librosa) (4.5.0)\n",
            "Requirement already satisfied: lazy-loader>=0.1 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.3)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.0.7)\n",
            "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.51.0->librosa) (0.41.1)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.10/dist-packages (from pooch>=1.0->librosa) (4.0.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from pooch>=1.0->librosa) (23.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from pooch>=1.0->librosa) (2.31.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->librosa) (3.2.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.10/dist-packages (from soundfile>=0.12.1->librosa) (1.16.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0->soundfile>=0.12.1->librosa) (2.21)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa) (2023.7.22)\n",
            "Collecting pydub\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Installing collected packages: pydub\n",
            "Successfully installed pydub-0.25.1\n",
            "Collecting pyrubberband\n",
            "  Downloading pyrubberband-0.3.0.tar.gz (4.1 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from pyrubberband) (1.16.0)\n",
            "Collecting pysoundfile>=0.8.0 (from pyrubberband)\n",
            "  Downloading PySoundFile-0.9.0.post1-py2.py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: cffi>=0.6 in /usr/local/lib/python3.10/dist-packages (from pysoundfile>=0.8.0->pyrubberband) (1.16.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=0.6->pysoundfile>=0.8.0->pyrubberband) (2.21)\n",
            "Building wheels for collected packages: pyrubberband\n",
            "  Building wheel for pyrubberband (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyrubberband: filename=pyrubberband-0.3.0-py3-none-any.whl size=4263 sha256=5cdd5ea793791844a980f39cae34bd60ed27f991cafccc95394c74ad1b1779e9\n",
            "  Stored in directory: /root/.cache/pip/wheels/b8/2d/f0/bb68fbfe67a42c858a79412321d28589218cbfe114c48ce664\n",
            "Successfully built pyrubberband\n",
            "Installing collected packages: pysoundfile, pyrubberband\n",
            "Successfully installed pyrubberband-0.3.0 pysoundfile-0.9.0.post1\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement stable_whisper (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for stable_whisper\u001b[0m\u001b[31m\n",
            "\u001b[0mCollecting romajitable\n",
            "  Downloading romajitable-0.0.1-py3-none-any.whl (13 kB)\n",
            "Installing collected packages: romajitable\n",
            "Successfully installed romajitable-0.0.1\n",
            "Collecting PySegmentKit\n",
            "  Downloading PySegmentKit-0.2.1-py3-none-any.whl (3.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.7/3.7 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PySegmentKit\n",
            "Successfully installed PySegmentKit-0.2.1\n"
          ]
        }
      ],
      "source": [
        "#import libraries\n",
        "!pip install -U git+https://github.com/jianfch/stable-ts.git\n",
        "!pip install alkana\n",
        "!pip install jaconv\n",
        "!pip install librosa\n",
        "!pip install pydub\n",
        "!pip install pyrubberband\n",
        "!pip install stable_whisper\n",
        "!pip install romajitable\n",
        "!pip install PySegmentKit"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt-get update -y\n",
        "!sudo apt-get install -y rubberband-cli"
      ],
      "metadata": {
        "id": "JveLlR8AyG75",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b5947b9c-f7fd-4e61-faa6-3a434fc491fd"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rGet:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,626 B]\n",
            "Get:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
            "Get:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [631 kB]\n",
            "Get:4 http://security.ubuntu.com/ubuntu jammy-security InRelease [110 kB]\n",
            "Hit:5 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [119 kB]\n",
            "Hit:7 https://ppa.launchpadcontent.net/c2d4u.team/c2d4u4.0+/ubuntu jammy InRelease\n",
            "Hit:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Get:10 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [1,456 kB]\n",
            "Hit:11 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:12 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:13 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [1,512 kB]\n",
            "Get:14 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,013 kB]\n",
            "Get:15 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [1,240 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [1,487 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,281 kB]\n",
            "Fetched 8,856 kB in 4s (2,246 kB/s)\n",
            "Reading package lists... Done\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following NEW packages will be installed:\n",
            "  rubberband-cli\n",
            "0 upgraded, 1 newly installed, 0 to remove and 17 not upgraded.\n",
            "Need to get 87.5 kB of archives.\n",
            "After this operation, 223 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 rubberband-cli amd64 2.0.0-2 [87.5 kB]\n",
            "Fetched 87.5 kB in 1s (88.4 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 1.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package rubberband-cli.\n",
            "(Reading database ... 120880 files and directories currently installed.)\n",
            "Preparing to unpack .../rubberband-cli_2.0.0-2_amd64.deb ...\n",
            "Unpacking rubberband-cli (2.0.0-2) ...\n",
            "Setting up rubberband-cli (2.0.0-2) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#hiragana to phonemes of Julius with delimiters\n",
        "def conv2julius_divide(s):\n",
        "    s = s.replace('う゛ぁ',' b a/')\n",
        "    s = s.replace('う゛ぃ',' b i/')\n",
        "    s = s.replace('う゛ぇ',' b e/')\n",
        "    s = s.replace('う゛ぉ',' b o/')\n",
        "    s = s.replace('う゛ゅ',' by u/')\n",
        "\n",
        "    s = s.replace('あぁ',' a a/')\n",
        "    s = s.replace('いぃ',' i i/')\n",
        "    s = s.replace('いぇ',' i e/')\n",
        "    s = s.replace('いゃ',' y a/')\n",
        "    s = s.replace('うぅ',' u:/')\n",
        "    s = s.replace('えぇ',' e e/')\n",
        "    s = s.replace('おぉ',' o:/')\n",
        "    s = s.replace('かぁ',' k a:/')\n",
        "    s = s.replace('きぃ',' k i:/')\n",
        "    s = s.replace('くぅ',' k u:/')\n",
        "    s = s.replace('くゃ',' ky a/')\n",
        "    s = s.replace('くゅ',' ky u/')\n",
        "    s = s.replace('くょ',' ky o/')\n",
        "    s = s.replace('けぇ',' k e:/')\n",
        "    s = s.replace('こぉ',' k o:/')\n",
        "    s = s.replace('がぁ',' g a:/')\n",
        "    s = s.replace('ぎぃ',' g i:/')\n",
        "    s = s.replace('ぐぅ',' g u:/')\n",
        "    s = s.replace('ぐゃ',' gy a/')\n",
        "    s = s.replace('ぐゅ',' gy u/')\n",
        "    s = s.replace('ぐょ',' gy o/')\n",
        "    s = s.replace('げぇ',' g e:/')\n",
        "    s = s.replace('ごぉ',' g o:/')\n",
        "    s = s.replace('さぁ',' s a:/')\n",
        "    s = s.replace('しぃ',' sh i:/')\n",
        "    s = s.replace('すぅ',' s u:/')\n",
        "    s = s.replace('すゃ',' sh a/')\n",
        "    s = s.replace('すゅ',' sh u/')\n",
        "    s = s.replace('すょ',' sh o/')\n",
        "    s = s.replace('せぇ',' s e:/')\n",
        "    s = s.replace('そぉ',' s o:/')\n",
        "    s = s.replace('ざぁ',' z a:/')\n",
        "    s = s.replace('じぃ',' j i:/')\n",
        "    s = s.replace('ずぅ',' z u:/')\n",
        "    s = s.replace('ずゃ',' zy a/')\n",
        "    s = s.replace('ずゅ',' zy u/')\n",
        "    s = s.replace('ずょ',' zy o/')\n",
        "    s = s.replace('ぜぇ',' z e:/')\n",
        "    s = s.replace('ぞぉ',' z o:/')\n",
        "    s = s.replace('たぁ',' t a:/')\n",
        "    s = s.replace('ちぃ',' ch i:/')\n",
        "    s = s.replace('つぁ',' ts a/')\n",
        "    s = s.replace('つぃ',' ts i/')\n",
        "    s = s.replace('つぅ',' ts u:/')\n",
        "    s = s.replace('つゃ',' ch a/')\n",
        "    s = s.replace('つゅ',' ch u/')\n",
        "    s = s.replace('つょ',' ch o/')\n",
        "    s = s.replace('つぇ',' ts e/')\n",
        "    s = s.replace('つぉ',' ts o/')\n",
        "    s = s.replace('てぇ',' t e:/')\n",
        "    s = s.replace('とぉ',' t o:/')\n",
        "    s = s.replace('だぁ',' d a:/')\n",
        "    s = s.replace('ぢぃ',' j i:/')\n",
        "    s = s.replace('づぅ',' d u:/')\n",
        "    s = s.replace('づゃ',' zy a/')\n",
        "    s = s.replace('づゅ',' zy u/')\n",
        "    s = s.replace('づょ',' zy o/')\n",
        "    s = s.replace('でぇ',' d e:/')\n",
        "    s = s.replace('どぉ',' d o:/')\n",
        "    s = s.replace('なぁ',' n a:/')\n",
        "    s = s.replace('にぃ',' n i:/')\n",
        "    s = s.replace('ぬぅ',' n u:/')\n",
        "    s = s.replace('ぬゃ',' ny a/')\n",
        "    s = s.replace('ぬゅ',' ny u/')\n",
        "    s = s.replace('ぬょ',' ny o/')\n",
        "    s = s.replace('ねぇ',' n e:/')\n",
        "    s = s.replace('のぉ',' n o:/')\n",
        "    s = s.replace('はぁ',' h a:/')\n",
        "    s = s.replace('ひぃ',' h i:/')\n",
        "    s = s.replace('ふぅ',' f u:/')\n",
        "    s = s.replace('ふゃ',' hy a/')\n",
        "    s = s.replace('ふゅ',' hy u/')\n",
        "    s = s.replace('ふょ',' hy o/')\n",
        "    s = s.replace('へぇ',' h e:/')\n",
        "    s = s.replace('ほぉ',' h o:/')\n",
        "    s = s.replace('ばぁ',' b a:/')\n",
        "    s = s.replace('びぃ',' b i:/')\n",
        "    s = s.replace('ぶぅ',' b u:/')\n",
        "    s = s.replace('ふゃ',' hy a/')\n",
        "    s = s.replace('ぶゅ',' by u/')\n",
        "    s = s.replace('ふょ',' hy o/')\n",
        "    s = s.replace('べぇ',' b e:/')\n",
        "    s = s.replace('ぼぉ',' b o:/')\n",
        "    s = s.replace('ぱぁ',' p a:/')\n",
        "    s = s.replace('ぴぃ',' p i:/')\n",
        "    s = s.replace('ぷぅ',' p u:/')\n",
        "    s = s.replace('ぷゃ',' py a/')\n",
        "    s = s.replace('ぷゅ',' py u/')\n",
        "    s = s.replace('ぷょ',' py o/')\n",
        "    s = s.replace('ぺぇ',' p e:/')\n",
        "    s = s.replace('ぽぉ',' p o:/')\n",
        "    s = s.replace('まぁ',' m a:/')\n",
        "    s = s.replace('みぃ',' m i:/')\n",
        "    s = s.replace('むぅ',' m u:/')\n",
        "    s = s.replace('むゃ',' my a/')\n",
        "    s = s.replace('むゅ',' my u/')\n",
        "    s = s.replace('むょ',' my o/')\n",
        "    s = s.replace('めぇ',' m e:/')\n",
        "    s = s.replace('もぉ',' m o:/')\n",
        "    s = s.replace('やぁ',' y a:/')\n",
        "    s = s.replace('ゆぅ',' y u:/')\n",
        "    s = s.replace('ゆゃ',' y a:/')\n",
        "    s = s.replace('ゆゅ',' y u:/')\n",
        "    s = s.replace('ゆょ',' y o:/')\n",
        "    s = s.replace('よぉ',' y o:/')\n",
        "    s = s.replace('らぁ',' r a:/')\n",
        "    s = s.replace('りぃ',' r i:/')\n",
        "    s = s.replace('るぅ',' r u:/')\n",
        "    s = s.replace('るゃ',' ry a/')\n",
        "    s = s.replace('るゅ',' ry u/')\n",
        "    s = s.replace('るょ',' ry o/')\n",
        "    s = s.replace('れぇ',' r e:/')\n",
        "    s = s.replace('ろぉ',' r o:/')\n",
        "    s = s.replace('わぁ',' w a:/')\n",
        "    s = s.replace('をぉ',' o:/')\n",
        "\n",
        "    s = s.replace('う゛',' b u/')\n",
        "    s = s.replace('でぃ',' d i/')\n",
        "    s = s.replace('でぇ',' d e:/')\n",
        "    s = s.replace('でゃ',' dy a/')\n",
        "    s = s.replace('でゅ',' dy u/')\n",
        "    s = s.replace('でょ',' dy o/')\n",
        "    s = s.replace('てぃ',' t i/')\n",
        "    s = s.replace('てぇ',' t e:/')\n",
        "    s = s.replace('てゃ',' ty a/')\n",
        "    s = s.replace('てゅ',' ty u/')\n",
        "    s = s.replace('てょ',' ty o/')\n",
        "    s = s.replace('すぃ',' s i/')\n",
        "    s = s.replace('ずぁ',' z u a/')\n",
        "    s = s.replace('ずぃ',' z i/')\n",
        "    s = s.replace('ずぅ',' z u/')\n",
        "    s = s.replace('ずゃ',' zy a/')\n",
        "    s = s.replace('ずゅ',' zy u/')\n",
        "    s = s.replace('ずょ',' zy o/')\n",
        "    s = s.replace('ずぇ',' z e/')\n",
        "    s = s.replace('ずぉ',' z o/')\n",
        "    s = s.replace('きゃ',' ky a/')\n",
        "    s = s.replace('きゅ',' ky u/')\n",
        "    s = s.replace('きょ',' ky o/')\n",
        "    s = s.replace('しゃ',' sh a/')\n",
        "    s = s.replace('しゅ',' sh u/')\n",
        "    s = s.replace('しぇ',' sh e/')\n",
        "    s = s.replace('しょ',' sh o/')\n",
        "    s = s.replace('ちゃ',' ch a/')\n",
        "    s = s.replace('ちゅ',' ch u/')\n",
        "    s = s.replace('ちぇ',' ch e/')\n",
        "    s = s.replace('ちょ',' ch o/')\n",
        "    s = s.replace('とぅ',' t u/')\n",
        "    s = s.replace('とゃ',' ty a/')\n",
        "    s = s.replace('とゅ',' ty u/')\n",
        "    s = s.replace('とょ',' ty o/')\n",
        "    s = s.replace('どぁ',' d o a/')\n",
        "    s = s.replace('どぅ',' d u/')\n",
        "    s = s.replace('どゃ',' dy a/')\n",
        "    s = s.replace('どゅ',' dy u/')\n",
        "    s = s.replace('どょ',' dy o/')\n",
        "    s = s.replace('どぉ',' d o:/')\n",
        "    s = s.replace('にゃ',' ny a/')\n",
        "    s = s.replace('にゅ',' ny u/')\n",
        "    s = s.replace('にょ',' ny o/')\n",
        "    s = s.replace('ひゃ',' hy a/')\n",
        "    s = s.replace('ひゅ',' hy u/')\n",
        "    s = s.replace('ひょ',' hy o/')\n",
        "    s = s.replace('みゃ',' my a/')\n",
        "    s = s.replace('みゅ',' my u/')\n",
        "    s = s.replace('みょ',' my o/')\n",
        "    s = s.replace('りゃ',' ry a/')\n",
        "    s = s.replace('りゅ',' ry u/')\n",
        "    s = s.replace('りょ',' ry o/')\n",
        "    s = s.replace('ぎゃ',' gy a/')\n",
        "    s = s.replace('ぎゅ',' gy u/')\n",
        "    s = s.replace('ぎょ',' gy o/')\n",
        "    s = s.replace('ぢぇ',' j e/')\n",
        "    s = s.replace('ぢゃ',' j a/')\n",
        "    s = s.replace('ぢゅ',' j u/')\n",
        "    s = s.replace('ぢょ',' j o/')\n",
        "    s = s.replace('じぇ',' j e/')\n",
        "    s = s.replace('じゃ',' j a/')\n",
        "    s = s.replace('じゅ',' j u/')\n",
        "    s = s.replace('じょ',' j o/')\n",
        "    s = s.replace('びゃ',' by a/')\n",
        "    s = s.replace('びゅ',' by u/')\n",
        "    s = s.replace('びょ',' by o/')\n",
        "    s = s.replace('ぴゃ',' py a/')\n",
        "    s = s.replace('ぴゅ',' py u/')\n",
        "    s = s.replace('ぴょ',' py o/')\n",
        "    s = s.replace('うぁ',' u a/')\n",
        "    s = s.replace('うぃ',' w i/')\n",
        "    s = s.replace('うぇ',' w e/')\n",
        "    s = s.replace('うぉ',' w o/')\n",
        "    s = s.replace('ふぁ',' f a/')\n",
        "    s = s.replace('ふぃ',' f i/')\n",
        "    s = s.replace('ふぅ',' f u/')\n",
        "    s = s.replace('ふゃ',' hy a/')\n",
        "    s = s.replace('ふゅ',' hy u/')\n",
        "    s = s.replace('ふょ',' hy o/')\n",
        "    s = s.replace('ふぇ',' f e/')\n",
        "    s = s.replace('ふぉ',' f o/')\n",
        "\n",
        "    s = s.replace('あ',' a/')\n",
        "    s = s.replace('い',' i/')\n",
        "    s = s.replace('う',' u/')\n",
        "    s = s.replace('え',' e/')\n",
        "    s = s.replace('お',' o/')\n",
        "    s = s.replace('か',' k a/')\n",
        "    s = s.replace('き',' k i/')\n",
        "    s = s.replace('く',' k u/')\n",
        "    s = s.replace('け',' k e/')\n",
        "    s = s.replace('こ',' k o/')\n",
        "    s = s.replace('さ',' s a/')\n",
        "    s = s.replace('し',' sh i/')\n",
        "    s = s.replace('す',' s u/')\n",
        "    s = s.replace('せ',' s e/')\n",
        "    s = s.replace('そ',' s o/')\n",
        "    s = s.replace('た',' t a/')\n",
        "    s = s.replace('ち',' ch i/')\n",
        "    s = s.replace('つ',' ts u/')\n",
        "    s = s.replace('て',' t e/')\n",
        "    s = s.replace('と',' t o/')\n",
        "    s = s.replace('な',' n a/')\n",
        "    s = s.replace('に',' n i/')\n",
        "    s = s.replace('ぬ',' n u/')\n",
        "    s = s.replace('ね',' n e/')\n",
        "    s = s.replace('の',' n o/')\n",
        "    s = s.replace('は',' h a/')\n",
        "    s = s.replace('ひ',' h i/')\n",
        "    s = s.replace('ふ',' f u/')\n",
        "    s = s.replace('へ',' h e/')\n",
        "    s = s.replace('ほ',' h o/')\n",
        "    s = s.replace('ま',' m a/')\n",
        "    s = s.replace('み',' m i/')\n",
        "    s = s.replace('む',' m u/')\n",
        "    s = s.replace('め',' m e/')\n",
        "    s = s.replace('も',' m o/')\n",
        "    s = s.replace('ら',' r a/')\n",
        "    s = s.replace('り',' r i/')\n",
        "    s = s.replace('る',' r u/')\n",
        "    s = s.replace('れ',' r e/')\n",
        "    s = s.replace('ろ',' r o/')\n",
        "    s = s.replace('が',' g a/')\n",
        "    s = s.replace('ぎ',' g i/')\n",
        "    s = s.replace('ぐ',' g u/')\n",
        "    s = s.replace('げ',' g e/')\n",
        "    s = s.replace('ご',' g o/')\n",
        "    s = s.replace('ざ',' z a/')\n",
        "    s = s.replace('じ',' j i/')\n",
        "    s = s.replace('ず',' z u/')\n",
        "    s = s.replace('ぜ',' z e/')\n",
        "    s = s.replace('ぞ',' z o/')\n",
        "    s = s.replace('だ',' d a/')\n",
        "    s = s.replace('ぢ',' j i/')\n",
        "    s = s.replace('づ',' z u/')\n",
        "    s = s.replace('で',' d e/')\n",
        "    s = s.replace('ど',' d o/')\n",
        "    s = s.replace('ば',' b a/')\n",
        "    s = s.replace('び',' b i/')\n",
        "    s = s.replace('ぶ',' b u/')\n",
        "    s = s.replace('べ',' b e/')\n",
        "    s = s.replace('ぼ',' b o/')\n",
        "    s = s.replace('ぱ',' p a/')\n",
        "    s = s.replace('ぴ',' p i/')\n",
        "    s = s.replace('ぷ',' p u/')\n",
        "    s = s.replace('ぺ',' p e/')\n",
        "    s = s.replace('ぽ',' p o/')\n",
        "    s = s.replace('や',' y a/')\n",
        "    s = s.replace('ゆ',' y u/')\n",
        "    s = s.replace('よ',' y o/')\n",
        "    s = s.replace('わ',' w a/')\n",
        "    s = s.replace('を',' o/')\n",
        "    s = s.replace('ん',' N/')\n",
        "    s = s.replace('っ',' q/')\n",
        "    if ('ー' in s):\n",
        "      s = s.replace(\"/ー\",\"ー\")\n",
        "      s = s.replace('ー',':/')\n",
        "\n",
        "    s = s.replace('ぁ',' a/')\n",
        "    s = s.replace('ぃ',' i/')\n",
        "    s = s.replace('ぅ',' u/')\n",
        "    s = s.replace('ぇ',' e/')\n",
        "    s = s.replace('ぉ',' o/')\n",
        "    s = s.replace('ゎ',' w a/')\n",
        "\n",
        "    s = s[1:]\n",
        "\n",
        "    s = re.sub(r':+', ':', s)\n",
        "\n",
        "    return s"
      ],
      "metadata": {
        "id": "s5g5Qgb86NNx"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import glob\n",
        "import stable_whisper\n",
        "from collections import OrderedDict\n",
        "import re\n",
        "import jaconv\n",
        "import alkana\n",
        "import librosa\n",
        "import time\n",
        "import soundfile as sf\n",
        "import shutil\n",
        "import pathlib\n",
        "from pydub import AudioSegment\n",
        "from pydub.silence import split_on_silence\n",
        "import wave\n",
        "import pyrubberband as pyrb\n",
        "from natsort import natsorted\n",
        "import romajitable\n",
        "from PySegmentKit import PySegmentKit, PSKError\n",
        "import alkana\n",
        "import re\n",
        "\n",
        "#calculating audio duration\n",
        "def get_audio_duration(file_path):\n",
        "  with wave.open(file_path,  'rb') as wr:\n",
        "    fn = wr.getnframes()\n",
        "    fr = wr.getframerate()\n",
        "    duration =  1.0 * fn / fr\n",
        "    return duration\n",
        "\n",
        "#adjusting duration of last words on julius .lab\n",
        "def adjust_max_sec(od,file_name):\n",
        "  data = od.popitem()\n",
        "  with wave.open(file_name,  'rb') as wr:\n",
        "      fr = wr.getframerate()\n",
        "      fn = wr.getnframes()\n",
        "      length = 1.0*fn/fr\n",
        "\n",
        "  od[data[0].replace(\".\",\"\")] = [data[1][0],length]\n",
        "  od.move_to_end(data[0].replace(\".\",\"\"))\n",
        "  print(od[data[0].replace(\".\",\"\")])\n",
        "\n",
        "  return od\n",
        "\n",
        "#speech recogniton using whisper & making phoneme dictionary\n",
        "def load_recognize_speech(file_name):\n",
        "  #load & recognition\n",
        "  files:list[str] = [file_name]\n",
        "  od = OrderedDict()\n",
        "  new_od = OrderedDict()\n",
        "  model = stable_whisper.load_model('base')\n",
        "  recoginzed_txt = \"\"\n",
        "  for i, file in enumerate(files):\n",
        "    print(\"## {}\".format(file))\n",
        "    result = model.transcribe(file, language= \"en\", verbose=True)\n",
        "    for word in result.all_words():\n",
        "\n",
        "      if(word.word[1:] in od):\n",
        "        od[word.word[1:]].append(word.start)\n",
        "        od[word.word[1:]].append(word.end)\n",
        "      else:\n",
        "        od[word.word[1:]] = [word.start, word.end]\n",
        "\n",
        "      recoginzed_txt = recoginzed_txt + word.word + \" \"\n",
        "\n",
        "  new_od = adjust_max_sec(od,file_name)\n",
        "  first_key = next(iter(new_od))\n",
        "  new_od[first_key] = [0.0,new_od[first_key][1]]\n",
        "  recoginzed_txt = recoginzed_txt[:-1]\n",
        "\n",
        "  return new_od,recoginzed_txt\n",
        "\n",
        "#converting from English to Japanese hiragana for a sentence\n",
        "def converter_en_to_jp_sen(txt):\n",
        "  txt_split = txt[:-1].split(\" \")\n",
        "  txt_split = list(filter(lambda x: x != \"\", txt_split))\n",
        "  katakana_txt_list = []\n",
        "  for i in txt_split:\n",
        "    if(alkana.get_kana(i) is None):#if there are no information in the dictionary, use rulebase\n",
        "      katakana_txt_list.append(romajitable.to_kana(i).katakana)\n",
        "    else:\n",
        "      katakana_txt_list.append(alkana.get_kana(i))\n",
        "\n",
        "  katakana_txt = \" \".join(katakana_txt_list)\n",
        "  kana_txt =  jaconv.kata2hira(katakana_txt)\n",
        "  kana_txt = re.sub(r'[^ぁ-んァ-ンー]', '',kana_txt)\n",
        "  kana_txt = kana_txt.replace(\"ゔ\",\"う゛\")\n",
        "  katakana_pho = jaconv.hiragana2julius(kana_txt)\n",
        "\n",
        "\n",
        "  return katakana_pho,kana_txt.replace(\" \",\"\")\n",
        "\n",
        "#converting from English to Japanese hiragana for a word\n",
        "def converter_en_to_jp_word(word):\n",
        "  if(alkana.get_kana(word) is None):\n",
        "    word =  romajitable.to_kana(word)\n",
        "    word = word.katakana\n",
        "  else:\n",
        "    word = alkana.get_kana(word)\n",
        "\n",
        "  word =  jaconv.kata2hira(word)\n",
        "  word = word.replace(\"ゔ\",\"う゛\")\n",
        "  word = re.sub(r'[^ぁ-んァ-ンー]', '',word)\n",
        "  word = conv2julius_divide(word)\n",
        "\n",
        "  return word\n",
        "\n",
        "#resampling for audio\n",
        "def resample_wav(input_path, output_path, target_sr=16000):\n",
        "    y, sr = librosa.load(input_path, sr=None)\n",
        "    y_resampled = librosa.resample(y, orig_sr=sr, target_sr=target_sr)\n",
        "    sf.write(output_path, y_resampled, target_sr, subtype=\"PCM_16\")\n",
        "\n",
        "#making dictionary from results of Julius\n",
        "def julius_analysis(lab_path):\n",
        "  data_dict = {}\n",
        "  with open(lab_path, \"r\") as file:\n",
        "            print(file)\n",
        "            for line in file:\n",
        "              start_time, end_time, phone = line.strip().split(' ')\n",
        "              if(phone in data_dict):\n",
        "                data_dict[phone] += [start_time,end_time]\n",
        "              else:\n",
        "                data_dict[phone] = [start_time,end_time]\n",
        "  return data_dict\n",
        "\n",
        "#conducting phoneme segmentation using Julius\n",
        "def julius(dir):\n",
        "  sk = PySegmentKit(dir,\n",
        "    disable_silence_at_ends=False,\n",
        "    leave_dict=False,\n",
        "    debug=False,\n",
        "    triphone=False,\n",
        "    input_mfcc=False)\n",
        "\n",
        "  try:\n",
        "      segmented = sk.segment()\n",
        "      print(\"julius-results\")\n",
        "      for result in segmented.keys():\n",
        "          print(\"=====Segmentation result of {}.wav=====\".format(result))\n",
        "          for begintime, endtime, unit in segmented[result]:\n",
        "              print(\"{:.7f} {:.7f} {}\".format(begintime, endtime, unit))\n",
        "\n",
        "  except PSKError as e:\n",
        "      print(e)\n",
        "\n",
        "\n",
        "#Making dictionary from Japanese phoneme & English phoneme\n",
        "def make_duration_jp(od_speech_a,od_speech_b):\n",
        "  time_list = []\n",
        "  new_od_speech_b = OrderedDict()\n",
        "  for i in od_speech_a.keys():\n",
        "    search_sentence_list = i.split(\"/\")\n",
        "    for phone in search_sentence_list:\n",
        "      search_phone_list = phone.split(\" \")\n",
        "      search_phone_list = list(filter(lambda x: x != \"\", search_phone_list))\n",
        "\n",
        "      for j in search_phone_list:\n",
        "        time_list += [float(od_speech_b[j][0]),float(od_speech_b[j][1])]\n",
        "        del od_speech_b[j][0:2]\n",
        "\n",
        "      min_data = min(time_list)\n",
        "      max_data = max(time_list)\n",
        "\n",
        "    new_od_speech_b[i] = [min_data,max_data]\n",
        "    time_list.clear()\n",
        "  first_key = next(iter(new_od_speech_b))\n",
        "  new_od_speech_b[first_key] = [float(od_speech_b[\"silB\"][0]),new_od_speech_b[first_key][1]]\n",
        "  last_key = next(reversed(new_od_speech_b))\n",
        "  #bug fixed\n",
        "  new_od_speech_b[last_key] = [new_od_speech_b[list(new_od_speech_b.keys())[-2]][1],float(od_speech_b[\"silE\"][1])]\n",
        "  return new_od_speech_b\n",
        "\n",
        "#Making audio files for each word\n",
        "def split_wav_usedict(input_file, output_folder, dict):\n",
        "    if not os.path.exists(output_folder):\n",
        "        os.makedirs(output_folder)\n",
        "    num = 0\n",
        "    for i in dict:\n",
        "      sourceAudio = AudioSegment.from_wav(input_file)\n",
        "      processedAudio =  sourceAudio[dict[i][0]*1000:dict[i][1]*1000]\n",
        "      processedAudio.export(output_folder + \"/\"+str(num)+\".wav\", format=\"wav\")\n",
        "      num += 1\n",
        "\n",
        "\n",
        "#Adjusting Japanese speech using English duration\n",
        "def align_speech(file_b,od_speech_a,od_speech_b):#発話区間調整関数\n",
        "  field_put_folder = \"/content/field\"\n",
        "  out_put_folder = \"/content/output\"\n",
        "\n",
        "  if not os.path.exists(out_put_folder):\n",
        "    os.makedirs(out_put_folder)\n",
        "\n",
        "  split_wav_usedict(file_b,field_put_folder,od_speech_b)\n",
        "  count = 0\n",
        "\n",
        "  for i in od_speech_b.keys():\n",
        "    file_path_a = field_put_folder + \"/\" + str(count) + \".wav\"\n",
        "    data_one = od_speech_a[i][1]\n",
        "    data_zero = od_speech_a[i][0]\n",
        "    target_duration_a =  data_one - data_zero\n",
        "    source_duration = get_audio_duration(file_path_a)\n",
        "    multi_num = source_duration/target_duration_a\n",
        "    print(multi_num)\n",
        "    y, sr   = librosa.load(file_path_a, sr=16000, mono=True)\n",
        "    y_fast = pyrb.time_stretch(y, sr, multi_num)\n",
        "    sf.write(file_path_a,y_fast,sr)\n",
        "\n",
        "    count += 1\n",
        "\n",
        "  for k in range(count):\n",
        "    if(k == 0):\n",
        "      audio = AudioSegment.from_wav( field_put_folder + \"/\" + str(k) + \".wav\")\n",
        "    else:\n",
        "      audio+=AudioSegment.from_wav( field_put_folder + \"/\" + str(k) + \".wav\")\n",
        "\n",
        "  shutil.rmtree( field_put_folder)\n",
        "  os.mkdir( field_put_folder)\n",
        "\n",
        "  audio.export(out_put_folder + \"/\" + str(file_b).split(\"/\")[-1], format=\"wav\")\n",
        "\n",
        "#Making a directory concisted of audio & text\n",
        "def make_dir(audio_path,txt_path):\n",
        "    output_directory = \"/content/path_to_output_directory\"\n",
        "    os.makedirs(output_directory, exist_ok=True)\n",
        "    shutil.copy(audio_path, os.path.join(output_directory, \"tem.wav\"))\n",
        "    shutil.copy(txt_path, os.path.join(output_directory, \"tem.txt\"))\n",
        "\n",
        "    return output_directory\n",
        "\n",
        "#Adjusting seconds in dictionaries\n",
        "def adjust_dict(dict_tem):\n",
        "  stack =  OrderedDict()\n",
        "  for key, value in dict_tem.items():\n",
        "    if not stack:\n",
        "      stack[key] = value\n",
        "    else:\n",
        "      first_key = next(iter(stack))\n",
        "      if not stack[first_key][1] == value[0]:\n",
        "        dict_tem[key] = [stack[first_key][1],value[1]]\n",
        "        stack.clear()\n",
        "      else:\n",
        "        stack.clear()\n",
        "  return dict_tem"
      ],
      "metadata": {
        "id": "xKLyNjpySQHp"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#main\n",
        "def main():\n",
        "    folder_path_us = \"/content/us\"\n",
        "    folder_path_ja = \"/content/ja\"\n",
        "    tem_path_txt_file = \"/content/tem.txt\"\n",
        "    file_list_jp = natsorted(os.listdir(folder_path_ja))\n",
        "    for k,filename_a in enumerate(natsorted(os.listdir(folder_path_us))):\n",
        "        file_path_us = os.path.join(folder_path_us, filename_a)\n",
        "        file_path_ja = os.path.join(folder_path_ja, file_list_jp[k])\n",
        "        print(file_path_us)\n",
        "        print(file_path_ja)\n",
        "\n",
        "        model = stable_whisper.load_model('base')\n",
        "\n",
        "        od_speech_us = OrderedDict()\n",
        "        od_speech_us,recognized_text_a = load_recognize_speech(file_path_us)\n",
        "\n",
        "        new_od_speech_us = OrderedDict()\n",
        "        for i in od_speech_us.keys():\n",
        "            new_key = converter_en_to_jp_word(i)\n",
        "            if(new_key in new_od_speech_us):\n",
        "              new_od_speech_us[new_key].append(od_speech_us[i][0])\n",
        "              new_od_speech_us[new_key].append(od_speech_us[i][1])\n",
        "            else:\n",
        "              new_od_speech_us[new_key] = od_speech_us[i]\n",
        "\n",
        "\n",
        "\n",
        "        japanaized_phone,kana_txt = converter_en_to_jp_sen(recognized_text_a)\n",
        "\n",
        "        f = open(tem_path_txt_file, 'w', encoding=\"utf-8\")\n",
        "        f.write(kana_txt)\n",
        "        f.close()\n",
        "\n",
        "        resample_wav(file_path_ja, file_path_ja, target_sr=16000)\n",
        "        output = make_dir(file_path_ja, tem_path_txt_file)\n",
        "        julius(output)\n",
        "        od_speech_ja = julius_analysis(output + \"/tem.lab\")\n",
        "        shutil.rmtree(output)\n",
        "\n",
        "        new_od_speech_ja = make_duration_jp(new_od_speech_us,od_speech_ja)\n",
        "        new_od_speech_us = adjust_dict(new_od_speech_us)\n",
        "        new_od_speech_ja = adjust_dict(new_od_speech_ja)\n",
        "\n",
        "\n",
        "        print(new_od_speech_us)\n",
        "        print(new_od_speech_ja)\n",
        "\n",
        "        align_speech(file_path_ja,new_od_speech_us,new_od_speech_ja)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZU0hXXz2GvFu",
        "outputId": "315d225a-203f-43cf-aff3-70cd8720f880"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/us/q1_lv0.wav\n",
            "/content/ja/q1_lv2.wav\n",
            "## /content/us/q1_lv0.wav\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/stable_whisper/whisper_word_level.py:224: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[00:00.060 --> 00:01.980]  The turtle move slowly across the garden path.\n",
            "-[00:00.060] -> [00:00.180] \" The\"\n",
            "-[00:00.180] -> [00:00.440] \" turtle\"\n",
            "-[00:00.440] -> [00:00.640] \" move\"\n",
            "-[00:00.640] -> [00:01.060] \" slowly\"\n",
            "-[00:01.060] -> [00:01.440] \" across\"\n",
            "-[00:01.440] -> [00:01.660] \" the\"\n",
            "-[00:01.660] -> [00:01.880] \" garden\"\n",
            "-[00:01.880] -> [00:01.980] \" path.\"\n",
            "\n",
            "[1.88, 2.496]\n",
            "/content/path_to_output_directory/tem.wav\n",
            "Result saved in \"/content/path_to_output_directory/tem.lab\".\n",
            "\n",
            "julius-results\n",
            "=====Segmentation result of /content/path_to_output_directory/tem.wav=====\n",
            "0.0000000 0.0725000 silB\n",
            "0.0725000 0.1125000 z\n",
            "0.1125000 0.1825000 a\n",
            "0.1825000 0.3325000 t\n",
            "0.3325000 0.3925000 a:\n",
            "0.3925000 0.4225000 t\n",
            "0.4225000 0.4725000 u\n",
            "0.4725000 0.5125000 r\n",
            "0.5125000 0.8025000 u\n",
            "0.8025000 0.9225000 m\n",
            "0.9225000 1.0225000 u:\n",
            "1.0225000 1.1025000 b\n",
            "1.1025000 1.3325000 u\n",
            "1.3325000 1.4825000 s\n",
            "1.4825000 1.5125000 u\n",
            "1.5125000 1.5425000 r\n",
            "1.5425000 1.7125000 o\n",
            "1.7125000 1.7425000 w\n",
            "1.7425000 1.7725000 a\n",
            "1.7725000 1.8025000 r\n",
            "1.8025000 1.8625000 u\n",
            "1.8625000 2.0025000 y\n",
            "2.0025000 2.0325000 u\n",
            "2.0325000 2.0625000 a\n",
            "2.0625000 2.1425000 k\n",
            "2.1425000 2.1725000 u\n",
            "2.1725000 2.2025000 r\n",
            "2.2025000 2.2925000 o\n",
            "2.2925000 2.4425000 s\n",
            "2.4425000 2.5125000 u\n",
            "2.5125000 2.5525000 z\n",
            "2.5525000 2.6925000 a\n",
            "2.6925000 2.8725000 g\n",
            "2.8725000 2.9525000 a:\n",
            "2.9525000 2.9925000 d\n",
            "2.9925000 3.0225000 e\n",
            "3.0225000 3.1425000 N\n",
            "3.1425000 3.2425000 p\n",
            "3.2425000 3.4725000 a\n",
            "3.4725000 3.5825000 s\n",
            "3.5825000 3.6125000 u\n",
            "3.6125000 3.6825000 silE\n",
            "<_io.TextIOWrapper name='/content/path_to_output_directory/tem.lab' mode='r' encoding='UTF-8'>\n",
            "OrderedDict([('z a/', [0.0, 0.18, 1.44, 1.66]), ('t a:/ t u/ r u/', [0.18, 0.44]), ('m u:/ b u/', [0.44, 0.64]), ('s u/ r o/ w a/ r u/ y u/', [0.64, 1.06]), ('a/ k u/ r o/ s u/', [1.06, 1.44]), ('g a:/ d e/ N/', [1.44, 1.88]), ('p a/ s u/', [1.88, 2.496])])\n",
            "OrderedDict([('z a/', [0.0, 0.1825]), ('t a:/ t u/ r u/', [0.1825, 0.8025]), ('m u:/ b u/', [0.8025, 1.3325]), ('s u/ r o/ w a/ r u/ y u/', [1.3325, 2.0325]), ('a/ k u/ r o/ s u/', [2.0325, 2.5125]), ('g a:/ d e/ N/', [2.5125, 3.1425]), ('p a/ s u/', [3.1425, 3.6825])])\n",
            "1.0138888888888888\n",
            "2.3846153846153846\n",
            "2.65\n",
            "1.6666666666666663\n",
            "1.2631578947368425\n",
            "1.431818181818182\n",
            "0.8766233766233765\n",
            "/content/us/q2_lv0.wav\n",
            "/content/ja/q2_lv1.wav\n",
            "## /content/us/q2_lv0.wav\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/stable_whisper/whisper_word_level.py:224: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[00:00.000 --> 00:01.540]  The sun is shining brightly today.\n",
            "-[00:00.000] -> [00:00.180] \" The\"\n",
            "-[00:00.180] -> [00:00.460] \" sun\"\n",
            "-[00:00.460] -> [00:00.580] \" is\"\n",
            "-[00:00.580] -> [00:00.860] \" shining\"\n",
            "-[00:00.860] -> [00:01.160] \" brightly\"\n",
            "-[00:01.160] -> [00:01.540] \" today.\"\n",
            "\n",
            "[1.16, 1.848]\n",
            "/content/path_to_output_directory/tem.wav\n",
            "Result saved in \"/content/path_to_output_directory/tem.lab\".\n",
            "\n",
            "julius-results\n",
            "=====Segmentation result of /content/path_to_output_directory/tem.wav=====\n",
            "0.0000000 0.0625000 silB\n",
            "0.0625000 0.0925000 z\n",
            "0.0925000 0.1225000 a\n",
            "0.1225000 0.2625000 s\n",
            "0.2625000 0.2925000 a\n",
            "0.2925000 0.3225000 N\n",
            "0.3225000 0.3525000 i\n",
            "0.3525000 0.3825000 z\n",
            "0.3825000 0.4125000 u\n",
            "0.4125000 0.5625000 sh\n",
            "0.5625000 0.7125000 a\n",
            "0.7125000 0.7425000 i\n",
            "0.7425000 0.7725000 n\n",
            "0.7725000 0.8225000 i\n",
            "0.8225000 0.8525000 N\n",
            "0.8525000 0.8825000 g\n",
            "0.8825000 0.9225000 u\n",
            "0.9225000 0.9525000 b\n",
            "0.9525000 0.9925000 u\n",
            "0.9925000 1.0225000 r\n",
            "1.0225000 1.0525000 i\n",
            "1.0525000 1.1125000 g\n",
            "1.1125000 1.1425000 u\n",
            "1.1425000 1.1725000 h\n",
            "1.1725000 1.2125000 o\n",
            "1.2125000 1.2425000 t\n",
            "1.2425000 1.3025000 o\n",
            "1.3025000 1.3325000 r\n",
            "1.3325000 1.3725000 u\n",
            "1.3725000 1.4025000 y\n",
            "1.4025000 1.4325000 u\n",
            "1.4325000 1.5125000 t\n",
            "1.5125000 1.5525000 u\n",
            "1.5525000 1.5825000 d\n",
            "1.5825000 1.7925000 e\n",
            "1.7925000 1.8825000 i\n",
            "1.8825000 2.0325000 silE\n",
            "<_io.TextIOWrapper name='/content/path_to_output_directory/tem.lab' mode='r' encoding='UTF-8'>\n",
            "OrderedDict([('z a/', [0.0, 0.18]), ('s a/ N/', [0.18, 0.46]), ('i/ z u/', [0.46, 0.58]), ('sh a/ i/ n i/ N/ g u/', [0.58, 0.86]), ('b u/ r i/ g u/ h o/ t o/ r u/ y u/', [0.86, 1.16]), ('t u/ d e/ i/', [1.16, 1.848])])\n",
            "OrderedDict([('z a/', [0.0, 0.1225]), ('s a/ N/', [0.1225, 0.3225]), ('i/ z u/', [0.3225, 0.4125]), ('sh a/ i/ n i/ N/ g u/', [0.4125, 0.9225]), ('b u/ r i/ g u/ h o/ t o/ r u/ y u/', [0.9225, 1.4325]), ('t u/ d e/ i/', [1.4325, 2.0325])])\n",
            "0.6805555555555556\n",
            "0.7142857142857143\n",
            "0.7500000000000003\n",
            "1.8214285714285714\n",
            "1.7000000000000004\n",
            "0.8720930232558137\n"
          ]
        }
      ]
    }
  ]
}